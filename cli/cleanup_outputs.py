"""Clean up processing outputs from participant directories.

This utility deletes all outputs generated by the acoustic emissions processing
software, allowing directories to be reset for re-processing during testing.

Removes:
- *_outputs/ directories (containing .pkl, _meta.json files)
- Synced/ directories (containing synced .pkl files and QC outputs)
- processing_log_*.xlsx files (regeneratable from database)
- knee_processing_log_*.xlsx files (regeneratable from database)
- Stomp detection visualization plots (.png files)

Does NOT remove:
- Source audio/biomechanics files
- Database records (managed via is_active soft-delete on re-processing)

Usage:
    ae-cleanup-outputs /path/to/studies
    ae-cleanup-outputs /path/to/studies/#1011
    ae-cleanup-outputs /path/to/studies --dry-run
"""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
import shutil

logger = logging.getLogger(__name__)


def cleanup_participant_outputs(
    participant_dir: Path,
    *,
    dry_run: bool = False,
) -> dict[str, int]:
    """Clean up all processing outputs from a participant directory.

    Args:
        participant_dir: Path to participant directory (e.g., "#1011").
        dry_run: If True, log what would be deleted without actually deleting.

    Returns:
        Dictionary with counts of items deleted: {
            "outputs_dirs": count,
            "synced_dirs": count,
            "png_files": count,
            "log_files": count,
            "total_bytes": bytes_freed,
        }
    """
    if not participant_dir.exists():
        raise FileNotFoundError(f"Participant directory not found: {participant_dir}")

    if not participant_dir.name.startswith("#"):
        logging.warning(
            f"Directory name '{participant_dir.name}' does not start with '#'. "
            "Are you sure this is a participant directory?"
        )

    stats = {
        "outputs_dirs": 0,
        "synced_dirs": 0,
        "png_files": 0,
        "log_files": 0,
        "total_bytes": 0,
    }

    # Process both Left Knee and Right Knee
    for knee_side in ["Left", "Right"]:
        knee_dir = participant_dir / f"{knee_side} Knee"
        if not knee_dir.exists():
            logging.debug(f"Knee directory not found: {knee_dir}")
            continue

        _cleanup_knee_directory(knee_dir, stats, dry_run=dry_run)

    return stats


def cleanup_study_directory(
    study_dir: Path,
    *,
    dry_run: bool = False,
    limit: int | None = None,
) -> None:
    """Clean up all participant directories in a study directory.

    Args:
        study_dir: Path to directory containing participant folders.
        dry_run: If True, log what would be deleted without actually deleting.
        limit: Maximum number of participants to process (for testing).
    """
    if not study_dir.exists():
        raise FileNotFoundError(f"Study directory not found: {study_dir}")

    # Find all participant directories (starting with #)
    participant_dirs = sorted([d for d in study_dir.iterdir() if d.is_dir() and d.name.startswith("#")])

    if not participant_dirs:
        logging.warning(f"No participant directories found in {study_dir}")
        return

    if limit:
        participant_dirs = participant_dirs[:limit]
        logging.info(f"Processing first {limit} participant(s)")

    total_stats = {
        "outputs_dirs": 0,
        "synced_dirs": 0,
        "png_files": 0,
        "log_files": 0,
        "total_bytes": 0,
        "participants": 0,
    }

    for participant_dir in participant_dirs:
        logging.info(f"\nCleaning participant: {participant_dir.name}")
        try:
            stats = cleanup_participant_outputs(
                participant_dir,
                dry_run=dry_run,
            )
            total_stats["outputs_dirs"] += stats["outputs_dirs"]
            total_stats["synced_dirs"] += stats["synced_dirs"]
            total_stats["png_files"] += stats["png_files"]
            total_stats["log_files"] += stats["log_files"]
            total_stats["total_bytes"] += stats["total_bytes"]
            total_stats["participants"] += 1
        except Exception as e:  # pylint: disable=broad-except
            logging.error(f"Error cleaning {participant_dir.name}: {e}")

    # Print summary
    mode = "[DRY RUN] " if dry_run else ""
    logging.info(f"\n{mode}Cleanup Summary:")
    logging.info(f"  Participants processed: {total_stats['participants']}")
    logging.info(f"  Outputs directories removed: {total_stats['outputs_dirs']}")
    logging.info(f"  Synced directories removed: {total_stats['synced_dirs']}")
    logging.info(f"  PNG files removed: {total_stats['png_files']}")
    logging.info(f"  Log files removed: {total_stats['log_files']}")
    logging.info(f"  Total space freed: {_format_bytes(total_stats['total_bytes'])}")


def _cleanup_knee_directory(
    knee_dir: Path,
    stats: dict[str, int],
    *,
    dry_run: bool = False,
) -> None:
    """Clean up outputs from a knee directory (Left or Right).

    Args:
        knee_dir: Path to knee directory (e.g., "Left Knee").
        stats: Statistics dictionary to update.
        dry_run: If True, don't actually delete anything.
    """
    if not knee_dir.is_dir():
        return

    # Delete knee-level master log file (knee_processing_log_*.xlsx)
    for knee_log_file in knee_dir.glob("knee_processing_log_*.xlsx"):
        size = knee_log_file.stat().st_size
        if dry_run:
            logging.info(f"[DRY RUN] Would delete knee log: {knee_log_file} ({_format_bytes(size)})")
        else:
            logging.info(f"Deleting knee log file: {knee_log_file} ({_format_bytes(size)})")
            try:
                knee_log_file.unlink()
            except Exception as e:
                logging.warning(f"Failed to delete knee log {knee_log_file}: {e}")
                continue
        stats["log_files"] += 1
        stats["total_bytes"] += size

    # Iterate through all subdirectories (maneuver folders)
    for maneuver_dir in knee_dir.iterdir():
        if not maneuver_dir.is_dir():
            continue

        # Delete *_outputs directories
        for outputs_dir in maneuver_dir.glob("*_outputs"):
            if outputs_dir.is_dir():
                size = _get_dir_size(outputs_dir)
                if dry_run:
                    logging.info(f"[DRY RUN] Would delete: {outputs_dir} ({_format_bytes(size)})")
                else:
                    logging.info(f"Deleting outputs directory: {outputs_dir} ({_format_bytes(size)})")
                    shutil.rmtree(outputs_dir)
                stats["outputs_dirs"] += 1
                stats["total_bytes"] += size

        # Delete Synced directory (handle legacy and current casing)
        for synced_dir in [maneuver_dir / "Synced", maneuver_dir / "synced"]:
            if synced_dir.exists() and synced_dir.is_dir():
                size = _get_dir_size(synced_dir)
                if dry_run:
                    logging.info(f"[DRY RUN] Would delete: {synced_dir} ({_format_bytes(size)})")
                else:
                    logging.info(f"Deleting Synced directory: {synced_dir} ({_format_bytes(size)})")
                    shutil.rmtree(synced_dir)
                stats["synced_dirs"] += 1
                stats["total_bytes"] += size

        # Delete any PNG files (stomp detection plots, etc.)
        for png_file in maneuver_dir.glob("*.png"):
            size = png_file.stat().st_size
            if dry_run:
                logging.info(f"[DRY RUN] Would delete: {png_file} ({_format_bytes(size)})")
            else:
                logging.info(f"Deleting plot file: {png_file} ({_format_bytes(size)})")
                png_file.unlink()
            stats["png_files"] += 1
            stats["total_bytes"] += size

        # Delete processing logs (Excel files)
        for log_file in maneuver_dir.glob("processing_log_*.xlsx"):
            size = log_file.stat().st_size
            if dry_run:
                logging.info(f"[DRY RUN] Would delete log: {log_file} ({_format_bytes(size)})")
            else:
                logging.info(f"Deleting log file: {log_file} ({_format_bytes(size)})")
                try:
                    log_file.unlink()
                except Exception as e:
                    logging.warning(f"Failed to delete log {log_file}: {e}")
                    continue
            stats["log_files"] += 1
            stats["total_bytes"] += size


def _get_dir_size(directory: Path) -> int:
    """Calculate total size of a directory in bytes.

    Args:
        directory: Path to directory.

    Returns:
        Total size in bytes.
    """
    total = 0
    try:
        for item in directory.rglob("*"):
            if item.is_file():
                total += item.stat().st_size
    except (OSError, PermissionError) as e:
        logging.warning(f"Error calculating size of {directory}: {e}")
    return total


def _format_bytes(bytes_count: int) -> str:
    """Format byte count as human-readable string.

    Args:
        bytes_count: Number of bytes.

    Returns:
        Formatted string (e.g., "1.5 MB").
    """
    for unit in ["B", "KB", "MB", "GB"]:
        if bytes_count < 1024.0:
            return f"{bytes_count:.1f} {unit}"
        bytes_count = int(bytes_count / 1024)
    return f"{bytes_count:.1f} TB"


def main() -> None:
    """Command-line interface for cleanup utility."""
    parser = argparse.ArgumentParser(description="Clean up processing outputs from participant directories")
    parser.add_argument(
        "path",
        type=Path,
        help="Path to participant directory or study directory containing multiple participants",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be deleted without actually deleting",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Maximum number of participants to process (for testing)",
    )
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Set logging level (default: INFO)",
    )

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(levelname)s: %(message)s",
    )

    path = args.path.resolve()

    # Determine if path is a single participant or study directory
    if path.name.startswith("#"):
        # Single participant directory
        logging.info(f"Cleaning single participant: {path.name}")
        stats = cleanup_participant_outputs(
            path,
            dry_run=args.dry_run,
        )
        mode = "[DRY RUN] " if args.dry_run else ""
        logging.info(f"\n{mode}Cleanup Summary:")
        logging.info(f"  Outputs directories removed: {stats['outputs_dirs']}")
        logging.info(f"  Synced directories removed: {stats['synced_dirs']}")
        logging.info(f"  PNG files removed: {stats['png_files']}")
        logging.info(f"  Log files removed: {stats['log_files']}")
        logging.info(f"  Total space freed: {_format_bytes(stats['total_bytes'])}")
    else:
        # Study directory with multiple participants
        logging.info(f"Cleaning study directory: {path}")
        cleanup_study_directory(
            path,
            dry_run=args.dry_run,
            limit=args.limit,
        )


if __name__ == "__main__":
    main()
